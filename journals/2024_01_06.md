- #100DaysToOffload
	- There are two articles that I have read recently that I really vibed with. [What We Got Right, What We Got Wrong](https://commandcenter.blogspot.com/2024/01/what-we-got-right-what-we-got-wrong.html) by Rob Pike and [Rust vs Go Better Together](https://spf13.com/p/rust-vs-go-better-together/) by a notable Go developer spf13.
	- Regardless if you are familiar with these languages or not, these blog posts are very well written and I would recommend you check them out.
	- What I really appreciate about the mindset of a Go programmer is that they are typically very matter of fact about programming. I don't think there is anyone who is going to defend Go's verbose syntax as their hill to die on, but there are some obvious advantages that are often over looked by an untrained eye.
	- > First, what's good and bad in a programming language is largely a matter of opinion rather than fact, despite the certainty with which many people argue about even the most trivial features of Go or any other language.
	-
- #golang/libraries https://github.com/sugarme/transformer
	- nolan wants me to do something with transformers
- #rust/libraries https://github.com/guillaume-be/rust-bert ai ml
-
- #ai/predictions
	- Here are core predictions:
	  
	  1. Multimodal models will power the next generation of specialized AI.
	- Transformer models have not shown limitations in solutions they support. GPT-3 showed the power of large (high parameter count) models in text use cases.
	- Openai's GPT-4 & Apple’s Ferret were given sight and can now analyze images to perform summarization and generation.
	  
	  2. Cost of AI inference will reach near-zero
	- Moore’s Law. Nvidia, AMD, Intel, Apple seeing 20-30% performance improvements every 18 months with new generations of hardware.
	- AI infrastructure companies racing to zero. Sheer # of players will drive competition and costs down. AWS, Azure, GCP, predibase, lambdalabs, paperspace, hyper stack, run-pod with dozens more with a google search
	- Old hardware is still good. GPU availability will continue to get better as new hardware comes online and old hardware discounted for maximum utilization.
	  
	  3. Running huge (100B+ parameter) models will run on commodity hardware
	- Mixtral 8x7b (46.7B parameters) runs faster than Llama 2 (70B parameters) aand is more accurate than GPT-3.5 (175B parameters)
	- Microsoft’s Phi-2 (2.7B parameters) model performs within 2% of models 3x its size (Mistral 7B)
	  
	  We are not directly competing with:
	- Companies whose business model is to rent you access to hardware.
	- Companies whose business model is to solely provide you developer tools
	  
	  We are:
	- building uses cases around pretrained multimodal models (Document Understanding, Robotics)
	  https://github.com/clovaai/donut
	  https://waymo.com/open/
	- using our expertise around building, training, and monitoring models to provide the lowest barrier of entry to fine-tuned multimodal models
	- using data synthesization techniques to create foundational models without being limited by access to data