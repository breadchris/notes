- #lunabrain/ideas
	- https://news.ycombinator.com/item?id=36787924
	- Langchain is so immature in this area - it doesn't even have a plugin for logging. When we hit production general availability, we realized how bad the assessment/evaluation/observability of this technology is. It is so early, and there are so many weird things about it compared to deterministic software.
	  What we ended up building looks something like:
	- We added a way to log which embedded chunks are being referenced by searches that result from a prompt, so you can see the relationships between prompts and the resultant search results. Unlike a direct query, this is ambiguous. And lots and lots of logging of what's going on and where it gets or more importantly where it doesn't look but should. In particular you want to see if/where it reaches for custom tool vs. just answering from the core LLM (usually undesireable).
	- We built a user facing thumbs up/down with reason mechanic that feeds into a separate table we use to improve results.
	- We added a way to trace entire conversations that includes both the user facing prompts, responses, as well as many internal logs generated by the agent chain. This way, we can see where in the stepped process it breaks down and improve it.
	- We built what amounts to a library for 'prompt testing' which is basically taking user prompts that leads to hits (that we want to maintain quality for over time) or misses (that failed due to something not being as good as it could), and we run those for each new build. Essentially it's just a file for each 'capability' (e.g. each agent) that contains a bunch of QA prompts and answers. It doesn't generate the same answer every time even with 0 temperature, so we have to manually eyeball test some of the answers for a new build to see if it is better, worse or the same. This has enabled a lot, like changing LLMs to see the effects, that was not possible without the structured unit-test-like approach to QA. We could probably have an LLM evaluate if the answers are worse/better/the same but haven't done it yet.
	- We look at all the prompt/response pairs that customers are sending through and use an LLM to summarize and assess the quality of the answers. It is remarkably good at this and has made it easier to identify which things are an issue.
	- We are using a vector DB which is weird and not ideal. Thankfully, large scale databases are starting to add support for vector type data, which we will move to as soon as we possibly can, because really the only difference is how well it indexes and reads/writes vector type data.
	- We have established a dashboard with metrics including obvious things like how many unique people used it each day and the overall messages. But less obvious things too like 'Resolves within 3 prompts' rate, which is a proxy for basically does it get the job done or do they constantly have to chase it down. Depending on the problem you're solving, similar metrics will end up being evolved.
	  There's probably other things the team are doing I'm not even read into yet as it's such an immature and fast moving process but these are the general areas.
	- work on writing first substack post https://bytesizedai.substack.com/publish/settings#website
- #lunabrain/ideas
	- https://www.ycombinator.com/launches/JFm-vellum-workflows-quickly-prototype-and-deploy-ai-chains
		- visually construct chains of llms
- #protoflow/work
	- what to work on?
	- I want to make a compelling demo, what does that look like?
	- people want to know about vector databases and how to search through content
	- still need to figure out what is going on with the protobuf enums in the form
	- the generator code should be a little more thought out, it is pretty confusing to look at
	- templates should be able to come from a folder