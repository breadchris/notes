- #blog/feedback
	- Nice article! Again the topic is quite interesting!
	- 1. The chapter "BM25 and Probabilistic Search" felt like a big sudden jump: "Instead of just looking at raw term frequencies, it factored in doc length and the rarity of terms" -> this got me surprised because so far we haven't mentioned frequencies of terms at all, instead it was just text search for a pattern. So it feel here like we skipped the whole chapter, and I had hard time fitting this mentally with what I read so far.
	- 2. "Full-Text Search Engines" chapter: Started nice, but then BM25 was mentioned and since I didn't get what it is, the rest of the sentence was lost to me. Also terms like "faceted search", "scoring", even "filtering" in this context, didn't mean much to me. You then provide `Solr` query, but I don't undersand it -> I could try to guess what it means, but I already fill like I am too lost  to try to figure it out. Providing a bit of explanation for those terms from before, and having BM25 explained better, that would help, and then also I think if you show this query from Solr, that it would be worth coloring it nicely and explaining each segmet what it means, even if very shortly (with one or two sentences, or with some tooltips on the image or something).
	- 3. "Semantic Search and Vector Search" chapter -> Ok so this is very interesting chapter, as it talks about shift from pattern matching to semantic searching, so I think this is very interesting. I understood it, but I already know what vector search is, and I do think it could be made a bit simpler. You could open with a simple example and say how those traditional methods can't match "car" with "automobile", or can't match "tree" with "oak", but semantic search can, because it understand the meaning behind those and can figure out how they corellate -> so some very easy start like that, with examles, and then you go deeper. Python example of `gensim` -> a bit out of blue, not sure what to do with it / what am I supposed to learn from that.
	  Annoy and Faiss -> you talk about them in past tense -> are they not used any more?
	- 4. The Age of Transformers -> The only thing I got from this is that they are kind of like semantic search but better? But I don't really know what "transformer-based models" are, nor what "context-aware search experience" means. I would consider reorganizing this chapter to also start with some kind of example, possibly comparing it to the pervious chapter, the Vector search, and showing how is this better, and then explaining the tech behind it. You don't have to teach me the details of the Transformers, but at least tell me shortly what those are so I can mentally position it -> for example you could tell me those are the latest generation of neural networks and that this is the tech that LLMs are based on (are they? I am not sure I am guessing this) and are all the hype today, and that they can work with context because ... . Code -> cool, but again it is somewhat sudden and I don't konw what to think about it. Maybe if it was connected to some example, or if you gave me ab etter idea of what I am looking for: "Look how simple it is", or smth else, whatever you wanted to convey with it.
	- 5. I get now, in "What should I use today" chapter, that we were going through stages of search algorithm evolution and that for each stage we observed the tools and libraries popular for it. I wasn't aware of that while reading previous chapters, and it would help if I knew of that, as I could mentally organize stuff. So explaining this sooner might help, plus you might want to add some chronology for each chapter, and you might remind us -> "then, after the vectors, emerged a new exciting tech in 2019: transformers! And with them, not only did we have semantics in search, but also got the context in which word is residing" (Making this up).
	- 6. Sparse vs Dense -> this chapter suddenly appeared after "What should I use today" which felt like the end of a blog post
	- man, your feedback is insane! thank you so much for taking the time to share this with me.
	- 1. Ah yeah i understand this confusion. The Fuzzy Searching section should have introduced the concept of "trigrams" which introduced the concept of term frequency. The link to pg_trgm (aka postgres trigram) talks about this.
	  2. Yeah that query is pretty dense to parse, especially the "^1.5" on the end. I would probably just pick a different visual all together. Looking at it now it looks pretty useless lol
	  3. Yeah there are some visuals that come to mind that would support this content a lot, as you mentioned. I included the code here because I thought it could be motivating for someone to try it out on their own when they see how simple it is. Perhaps this would serve better if they were actually runnable code examples.
	  Past tense is a typo here, they are still relevant.
	  4. I wanted to make a distinction between OG vector search and what we have unlocked with transformers. The level of contextual awareness that you can get with transformers doesn't compare with things like word2vec. I think what would work well here is a comparison of this point with some visual/code being run.
	  5. That makes sense. I spent more of an effort on this post trying to set expectations about what content I am going to introduce to the reader (something I picked up from your past feedback) and will continue to improve on that. I feel like I need to have a DAG of concepts to help me keep track of how sections are related to each other. And maybe using an LLM to validate that those sections are indeed related once I actually write the content...
	  6. Yeah I agree, weird flow of content is going on here. A problem of having too much to say and wanting to get it all in. This blog post started as a completely different topic and i realized I needed to split that content out into a future post lol
	- I also felt that the start of the blog was pretty strong and then tapered off by the end. I should challenge myself to do an Explain it like I am 5 post and then have an LLM validate that a 5 year old would be able to understand it lol
	- Thank you again for the feedback, it means a lot that you would take the time to review my content. If you have anything that you are interested in me writing about please let me know ðŸ™‚
- #lunabrain/work
	- http://static.wiki/ database of wikipedia entries that can be used when parsing transcriptions and link to stuff