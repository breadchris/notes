- #100DaysToOffload
- #golang/libraries https://github.com/sugarme/transformer
	- nolan wants me to do something with transformers
- #ai/predictions
	- Here are core predictions:
	  
	  1. Multimodal models will power the next generation of specialized AI.
	- Transformer models have not shown limitations in solutions they support. GPT-3 showed the power of large (high parameter count) models in text use cases.
	- Openai's GPT-4 & Apple’s Ferret were given sight and can now analyze images to perform summarization and generation.
	  
	  2. Cost of AI inference will reach near-zero
	- Moore’s Law. Nvidia, AMD, Intel, Apple seeing 20-30% performance improvements every 18 months with new generations of hardware.
	- AI infrastructure companies racing to zero. Sheer # of players will drive competition and costs down. AWS, Azure, GCP, predibase, lambdalabs, paperspace, hyper stack, run-pod with dozens more with a google search
	- Old hardware is still good. GPU availability will continue to get better as new hardware comes online and old hardware discounted for maximum utilization.
	  
	  3. Running huge (100B+ parameter) models will run on commodity hardware
	- Mixtral 8x7b (46.7B parameters) runs faster than Llama 2 (70B parameters) aand is more accurate than GPT-3.5 (175B parameters)
	- Microsoft’s Phi-2 (2.7B parameters) model performs within 2% of models 3x its size (Mistral 7B)
	  
	  We are not directly competing with:
	- Companies whose business model is to rent you access to hardware.
	- Companies whose business model is to solely provide you developer tools
	  
	  We are:
	- building uses cases around pretrained multimodal models (Document Understanding, Robotics)
	  https://github.com/clovaai/donut
	  https://waymo.com/open/
	- using our expertise around building, training, and monitoring models to provide the lowest barrier of entry to fine-tuned multimodal models
	- using data synthesization techniques to create foundational models without being limited by access to data